{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import sys\n",
    "from glob import glob\n",
    "import gc\n",
    "\n",
    "sys.path.append('../../')\n",
    "\"\"\"\n",
    "local scripts, if loading from a different directory include that with a '.' between\n",
    "directory name and script name\n",
    "\"\"\"\n",
    "from tropical_PODs.PODs.POD_utils import calculate_saturation_specific_humidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define input directories and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years to analyze\n",
    "start_year = (2010)\n",
    "end_year = (2010)\n",
    "\n",
    "################\n",
    "###.  ERA5.  ###\n",
    "################\n",
    "\n",
    "# Atmosphere\n",
    "\n",
    "ifile_specific_humidity = '../../tropical_PODs/data/shum.2p5.*.nc' # ERA5 Specific Humidity\n",
    "ifile_temperature = '../../tropical_PODs/data/air.2p5.*.nc' # ERA5 Temperature\n",
    "ifile_surface_pressure = '../../tropical_PODs/data/pres.sfc.2p5.*.nc' # ERA5 Surface Pressure\n",
    "ifile_precipitation = '../../tropical_PODs/data/3B-DAY.MS.MRG.3IMERG.V06.*' # IMERG Precipitation\n",
    "\n",
    "# Land\n",
    "ifile_land_frac = '../../tropical_PODs/data/land_sea_mask.erai.2p5.nc' # ERAi Land Fraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define output directories and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for datasets string list\n",
    "odir_datasets_string_list = ['../../tropical_PODs/examples/ofiles_examples/'] # ERA5 2p5_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "    \n",
    "g = 9.8 # [m s^-2]\n",
    "\n",
    "#########################################\n",
    "# Define paths of files we wish to load #\n",
    "#########################################\n",
    "    \n",
    "# glob expands paths with * to a list of files, like the unix shell #\n",
    "\n",
    "paths_specific_humidity = glob(ifile_specific_humidity)\n",
    "paths_temperature = glob(ifile_temperature)\n",
    "paths_surface_pressure = glob(ifile_surface_pressure)\n",
    "paths_precipitation = glob(ifile_precipitation)\n",
    "paths_land = glob(ifile_land_frac)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n"
     ]
    }
   ],
   "source": [
    "for year in range(start_year, end_year + 1):\n",
    "        \n",
    "    print(year)\n",
    "    \n",
    "    # Define year strings #\n",
    "        \n",
    "    current_year_string = str(year)\n",
    "            \n",
    "    # Limit paths #\n",
    "        \n",
    "    year_limited_paths_specific_humidity = []\n",
    "    year_limited_paths_temperature = []\n",
    "    year_limited_paths_surface_pressure = []\n",
    "    year_limited_paths_precipitation = []\n",
    "            \n",
    "    for string in paths_specific_humidity:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_specific_humidity += [string]\n",
    "            \n",
    "    for string in paths_temperature:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_temperature += [string]\n",
    "            \n",
    "    for string in paths_surface_pressure:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_surface_pressure += [string]\n",
    "                \n",
    "    for string in paths_precipitation:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_precipitation += [string]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:\nhttps://docs.xarray.dev/en/stable/user-guide/io.html \nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/bwolding_personal/My Drive/Work/Github/tropical_PODs/testing/initial_testing.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bwolding_personal/My%20Drive/Work/Github/tropical_PODs/testing/initial_testing.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataset_temperature \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mopen_mfdataset(year_limited_paths_temperature, combine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mby_coords\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bwolding_personal/My%20Drive/Work/Github/tropical_PODs/testing/initial_testing.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dataset_surface_pressure \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mopen_dataset(year_limited_paths_surface_pressure[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bwolding_personal/My%20Drive/Work/Github/tropical_PODs/testing/initial_testing.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dataset_precipitation \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mopen_mfdataset(year_limited_paths_precipitation, combine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mby_coords\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bwolding_personal/My%20Drive/Work/Github/tropical_PODs/testing/initial_testing.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m dataset_land \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39mopen_dataset(paths_land[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:1038\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     open_ \u001b[39m=\u001b[39m open_dataset\n\u001b[1;32m   1036\u001b[0m     getattr_ \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m\n\u001b[0;32m-> 1038\u001b[0m datasets \u001b[39m=\u001b[39m [open_(p, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m paths]\n\u001b[1;32m   1039\u001b[0m closers \u001b[39m=\u001b[39m [getattr_(ds, \u001b[39m\"\u001b[39m\u001b[39m_close\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m ds \u001b[39min\u001b[39;00m datasets]\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m preprocess \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     open_ \u001b[39m=\u001b[39m open_dataset\n\u001b[1;32m   1036\u001b[0m     getattr_ \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m\n\u001b[0;32m-> 1038\u001b[0m datasets \u001b[39m=\u001b[39m [open_(p, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m paths]\n\u001b[1;32m   1039\u001b[0m closers \u001b[39m=\u001b[39m [getattr_(ds, \u001b[39m\"\u001b[39m\u001b[39m_close\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m ds \u001b[39min\u001b[39;00m datasets]\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m preprocess \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:547\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    546\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m     engine \u001b[39m=\u001b[39m plugins\u001b[39m.\u001b[39mguess_engine(filename_or_obj)\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m from_array_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     from_array_kwargs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/plugins.py:197\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfound the following matches with the input file in xarray\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms IO \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbackends: \u001b[39m\u001b[39m{\u001b[39;00mcompatible_engines\u001b[39m}\u001b[39;00m\u001b[39m. But their dependencies may not be installed, see:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n\u001b[0;32m--> 197\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:\nhttps://docs.xarray.dev/en/stable/user-guide/io.html \nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html"
     ]
    }
   ],
   "source": [
    " \n",
    "    #####################\n",
    "    ####  Load Data  ####\n",
    "    #####################\n",
    "\n",
    "    # Data is \"lazy loaded\", nothing is actually loaded until we \"look\" at data in some way #\n",
    "\n",
    "    dataset_specific_humidity = xr.open_mfdataset(year_limited_paths_specific_humidity, combine=\"by_coords\")\n",
    "    dataset_temperature = xr.open_mfdataset(year_limited_paths_temperature, combine=\"by_coords\")\n",
    "    dataset_surface_pressure = xr.open_dataset(year_limited_paths_surface_pressure[0])\n",
    "    dataset_precipitation = xr.open_mfdataset(year_limited_paths_precipitation, combine=\"by_coords\")\n",
    "    dataset_land = xr.open_dataset(paths_land[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #####################\n",
    "    ####  Load Data  ####\n",
    "    #####################\n",
    "              \n",
    "    # Make data arrays, loading only the year of interest #\n",
    "    full_lat = dataset_surface_pressure['lat']\n",
    "    full_lon = dataset_surface_pressure['lon']\n",
    "    land_sea_mask = dataset_land['land_sea_mask']\n",
    "\n",
    "    PS = dataset_surface_pressure['pres'].sel(time = slice(str(year)+'-01-01', str(year)+'-12-31'), lat = slice(15, -15)) # [Pa]\n",
    "    Q = dataset_specific_humidity['shum'].sel(time = slice(str(year)+'-01-01', str(year)+'-12-31'),lat = slice(15, -15), level = slice(70, 1000)) # [Kg/Kg]\n",
    "    T = dataset_temperature['air'].sel(time = slice(str(year)+'-01-01', str(year)+'-12-31'),lat = slice(15, -15), level = slice(70, 1000)) # [K]\n",
    "\n",
    "    # Actually load data #\n",
    "    land_sea_mask.load()\n",
    "    PS.load()\n",
    "    Q.load()\n",
    "    T.load()\n",
    "\n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ################################\n",
    "    ####  Average Data to Daily ####\n",
    "    ################################\n",
    "    \n",
    "    ###   Test for Averaging Method   ###\n",
    "            \n",
    "    #PS.sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).plot()\n",
    "    #print(PS.resample(time='1D').mean('time').sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75))\n",
    "    #print(PS.sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).mean('time'))\n",
    "    #print(PS.resample(time='1D').mean('time').sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).values == PS.sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).mean('time').values)\n",
    "    \n",
    "    ###   Perform Averaging   ###\n",
    "            \n",
    "    PS = PS.resample(time='1D').mean('time')\n",
    "    Q = Q.resample(time='1D').mean('time')\n",
    "    T = T.resample(time='1D').mean('time')\n",
    "\n",
    "    ### Update time to reflect center of daily average   ###\n",
    "    \n",
    "    PS = PS.assign_coords({'time':PS['time']+pd.to_timedelta(10.5, unit='H')})\n",
    "    Q = Q.assign_coords({'time':Q['time']+pd.to_timedelta(10.5, unit='H')})\n",
    "    T = T.assign_coords({'time':T['time']+pd.to_timedelta(10.5, unit='H')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ###############################################\n",
    "    ####  Modify \"landfrac\" Variable as Needed ####\n",
    "    ###############################################\n",
    "    \n",
    "    print(\"Modifying landfrac as needed\")\n",
    "\n",
    "    landfrac = land_sea_mask.rename({'Latitude':'lat','Longitude':'lon'})\n",
    "    landfrac = landfrac.rename('landfrac')\n",
    "    print(landfrac)\n",
    "    \n",
    "    # The landfrac variable does not have lat/lon coordinates. Assign those of variables and check to make sure they make sense #\n",
    "    \n",
    "    #print(landfrac.coords['lat'])\n",
    "    landfrac.coords['lat'] = full_lat.coords['lat']\n",
    "    landfrac.coords['lon'] = full_lon.coords['lon']\n",
    "\n",
    "    landfrac = landfrac.transpose()\n",
    "    \n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "    \n",
    "    #####################################\n",
    "    ####  Modify variables as needed ####\n",
    "    #####################################\n",
    "    \n",
    "    PS = PS.rename('PS')\n",
    "    PS = PS.transpose('time','lat','lon')\n",
    "    PS = PS.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    #print(PS)\n",
    "    \n",
    "    Q = Q.rename({'level':'lev'})\n",
    "    Q = Q.rename('Q')\n",
    "    Q = Q.transpose('time','lev','lat','lon')\n",
    "    Q = Q.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    #print(Q)\n",
    "    \n",
    "    T = T.rename({'level':'lev'})\n",
    "    T = T.rename('T')\n",
    "    T = T.transpose('time','lev','lat','lon')\n",
    "    T = T.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    #print(T)\n",
    "    \n",
    "    landfrac = landfrac.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    \n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "\n",
    "    #########################################\n",
    "    ####  Calculate True Model Pressure  ####\n",
    "    #########################################\n",
    "\n",
    "    print(\"Calculating true model pressure\")\n",
    "    \n",
    "    # Set upper most interface equal to uppermost level midpoint, and lowest interface equal to surface pressure.\n",
    "    # This will still permit the desired vertical integral, just choose appropriate upper and lower integration limits\n",
    "    \n",
    "    # Model level midpoint\n",
    "\n",
    "    true_pressure_midpoint = Q['lev'] * 100. # To convert to Pa\n",
    "    true_pressure_midpoint = true_pressure_midpoint.rename('true_pressure_midpoint_Pa')\n",
    "    true_pressure_midpoint = true_pressure_midpoint.expand_dims({'lat':Q['lat'], 'lon':Q['lon'], 'time':Q['time']})\n",
    "    true_pressure_midpoint = true_pressure_midpoint.transpose('time','lev','lat','lon')\n",
    "    \n",
    "    # Model level interfaces\n",
    "    \n",
    "    true_pressure_interface = np.empty((len(Q.time),len(Q.lat),len(Q.lon),len(Q.lev)+1))\n",
    "\n",
    "    for interface_level_counter in range(len(Q.lev) + 1):\n",
    "        if interface_level_counter == 0:\n",
    "            true_pressure_interface[:,:,:,interface_level_counter] = Q['lev'].isel(lev=0).values # Set upper most interface equal to uppermost level midpoint\n",
    "        elif interface_level_counter == (len(Q.lev)):\n",
    "            true_pressure_interface[:,:,:,interface_level_counter] = PS # Set lowest interface equal to surface pressure\n",
    "        else:\n",
    "            true_pressure_interface[:,:,:,interface_level_counter] = (Q['lev'].isel(lev=interface_level_counter-1).values + Q['lev'].isel(lev=interface_level_counter).values) / 2.,  # Set middle interfaces equal to half way points between level midpoints\n",
    "            \n",
    "    coords = {'time':Q['time'], 'lat':Q['lat'], 'lon':Q['lon'], 'ilev':np.arange(1,len(Q.lev) + 2)}\n",
    "    dims = ['time', 'lat', 'lon', 'ilev']\n",
    "    true_pressure_interface = xr.DataArray(true_pressure_interface,dims=dims,coords=coords) * 100. # To convert to Pa\n",
    "    true_pressure_interface.attrs['units'] = 'Pa'      \n",
    "    \n",
    "    true_pressure_interface = true_pressure_interface.transpose('time','ilev','lat','lon')\n",
    "\n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "\n",
    "    ###############################################\n",
    "    ####  Limit to Oceanic (<10% Land) Points  ####\n",
    "    ###############################################\n",
    "        \n",
    "    print('Applying Land/Ocean Mask')\n",
    "        \n",
    "    # Create ocean mask #\n",
    "\n",
    "    is_valid_ocean_mask = (landfrac.sel(lat = slice(-15, 15)) < 0.1)\n",
    "\n",
    "    #is_valid_ocean_mask.plot()\n",
    "\n",
    "    # Apply ocean mask to appropriate variables, setting invalid locations to nan #\n",
    "\n",
    "    PS_ocean = PS.where(is_valid_ocean_mask, other = np.nan)\n",
    "    \n",
    "    #####################################################\n",
    "    ####  Instantiate Virtual Temperature Variables  ####\n",
    "    #####################################################\n",
    "    \n",
    "    nan_data = np.zeros(np.shape(Q))\n",
    "    nan_data[:] = np.nan\n",
    "    coords = Q.coords\n",
    "    dims = Q.dims\n",
    "    \n",
    "    temp_v_env_all_times = xr.DataArray(nan_data,coords=coords,dims=dims)\n",
    "    temp_v_plume_DIB_all_times = xr.full_like(temp_v_env_all_times,np.nan) # can't simply repeat use of nan_data, otherwise pointer indicates same variable\n",
    "    temp_v_plume_DIBDBL_all_times = xr.full_like(temp_v_env_all_times,np.nan) # can't simply repeat use of nan_data, otherwise pointer indicates same variable\n",
    "    temp_v_plume_NOMIX_all_times = xr.full_like(temp_v_env_all_times,np.nan)\n",
    " \n",
    "    # Name variables #\n",
    "\n",
    "    temp_v_env_all_times.name = 'temp_v_env'\n",
    "    temp_v_plume_DIB_all_times.name = 'temp_v_plume_DIB'\n",
    "    temp_v_plume_DIBDBL_all_times.name = 'temp_v_plume_DIBDBL'\n",
    "    temp_v_plume_NOMIX_all_times.name = 'temp_v_plume_NOMIX'\n",
    "    \n",
    "    # Add desired attributes #\n",
    "    \n",
    "    temp_v_env_all_times.attrs['Units'] = '[K]'\n",
    "    temp_v_plume_DIB_all_times.attrs['Units'] = '[K]'\n",
    "    temp_v_plume_DIBDBL_all_times.attrs['Units'] = '[K]'\n",
    "    temp_v_plume_NOMIX_all_times.attrs['Units'] = '[K]'\n",
    "    \n",
    "    ######################################\n",
    "    ####  Instantiate CAPE Variables  ####\n",
    "    ######################################\n",
    "    \n",
    "    nan_data = np.zeros(np.shape(PS))\n",
    "    nan_data[:] = np.nan\n",
    "    coords = PS.coords\n",
    "    dims = PS.dims\n",
    " \n",
    "    CAPE_DIB_1000_to_600 = xr.DataArray(nan_data,coords=coords,dims=dims)\n",
    "    CAPE_DIBDBL_1000_to_600 = xr.full_like(CAPE_DIB_1000_to_600,np.nan) # can't simply repeat use of nan_data, otherwise pointer indicates same variable\n",
    "    CAPE_NOMIX_1000_to_600 = xr.full_like(CAPE_DIB_1000_to_600,np.nan) # can't simply repeat use of nan_data, otherwise pointer indicates same variable\n",
    "    \n",
    "    CAPE_DIB_1000_to_850 = xr.full_like(CAPE_DIB_1000_to_600,np.nan)\n",
    "    CAPE_DIBDBL_1000_to_850 = xr.full_like(CAPE_DIB_1000_to_600,np.nan)\n",
    "    CAPE_NOMIX_1000_to_850 = xr.full_like(CAPE_DIB_1000_to_600,np.nan)\n",
    "    \n",
    "    CAPE_DIB_850_to_600 = xr.full_like(CAPE_DIB_1000_to_600,np.nan)\n",
    "    CAPE_DIBDBL_850_to_600 = xr.full_like(CAPE_DIB_1000_to_600,np.nan)\n",
    "    CAPE_NOMIX_850_to_600 = xr.full_like(CAPE_DIB_1000_to_600,np.nan)\n",
    "    \n",
    "    # Name variables #\n",
    "\n",
    "    CAPE_DIB_1000_to_600.name = 'CAPE_DIB_1000_to_600'\n",
    "    CAPE_DIBDBL_1000_to_600.name = 'CAPE_DIBDBL_1000_to_600'\n",
    "    CAPE_NOMIX_1000_to_600.name = 'CAPE_NOMIX_1000_to_600'\n",
    "    \n",
    "    CAPE_DIB_1000_to_850.name = 'CAPE_DIB_1000_to_850'\n",
    "    CAPE_DIBDBL_1000_to_850.name = 'CAPE_DIBDBL_1000_to_850'\n",
    "    CAPE_NOMIX_1000_to_850.name = 'CAPE_NOMIX_1000_to_850'\n",
    "    \n",
    "    CAPE_DIB_850_to_600.name = 'CAPE_DIB_850_to_600'\n",
    "    CAPE_DIBDBL_850_to_600.name = 'CAPE_DIBDBL_850_to_600'\n",
    "    CAPE_NOMIX_850_to_600.name = 'CAPE_NOMIX_850_to_600'\n",
    "    \n",
    "    # Add desired attributes #\n",
    "\n",
    "    CAPE_DIB_1000_to_600.attrs['Units'] = '[J Kg^-1]'\n",
    "    CAPE_DIBDBL_1000_to_600.attrs['Units'] = '[J Kg^-1]'\n",
    "    CAPE_NOMIX_1000_to_600.attrs['Units'] = '[J Kg^-1]'\n",
    "    \n",
    "    CAPE_DIB_1000_to_850.attrs['Units'] = '[J Kg^-1]'\n",
    "    CAPE_DIBDBL_1000_to_850.attrs['Units'] = '[J Kg^-1]'\n",
    "    CAPE_NOMIX_1000_to_850.attrs['Units'] = '[J Kg^-1]'\n",
    "    \n",
    "    CAPE_DIB_850_to_600.attrs['Units'] = '[J Kg^-1]'\n",
    "    CAPE_DIBDBL_850_to_600.attrs['Units'] = '[J Kg^-1]'\n",
    "    CAPE_NOMIX_850_to_600.attrs['Units'] = '[J Kg^-1]'\n",
    "\n",
    "    ####################################\n",
    "    ####  Calculate Buoyancy Terms  ####\n",
    "    ####################################\n",
    "    \n",
    "    launch_level_hPa = 1000 # [hPa] per Fiaz's code. Must remain close to 1000 to maintain mass flux profile\n",
    "    \n",
    "    for latitude in CAPE_DIB_1000_to_600.lat:\n",
    "        \n",
    "        print(latitude)\n",
    "        \n",
    "        for longitude in CAPE_DIB_1000_to_600.lon:\n",
    "            \n",
    "            if is_valid_ocean_mask.sel(lat=latitude,lon=longitude):\n",
    "                \n",
    "                ########################################\n",
    "                ####  Virtual Temperature Variables ####\n",
    "                ########################################\n",
    "                \n",
    "                temp_v_env, temp_v_plume_DIB, temp_v_plume_NOMIX, temp_v_plume_DIBDBL = numerical_plume_model(T.sel(lat=latitude,lon=longitude), Q.sel(lat=latitude,lon=longitude), 1000)\n",
    "                \n",
    "                temp_v_env_all_times.loc[dict(lat=latitude,lon=longitude)] = temp_v_env\n",
    "                temp_v_plume_DIB_all_times.loc[dict(lat=latitude,lon=longitude)] = temp_v_plume_DIB\n",
    "                temp_v_plume_DIBDBL_all_times.loc[dict(lat=latitude,lon=longitude)] = temp_v_plume_DIBDBL\n",
    "                temp_v_plume_NOMIX_all_times.loc[dict(lat=latitude,lon=longitude)] = temp_v_plume_NOMIX\n",
    "\n",
    "                ##########################\n",
    "                ####  CAPE Variables  ####\n",
    "                ##########################\n",
    "    \n",
    "                # print('Calculating CAPE Variables')\n",
    "                \n",
    "                CAPE_DIB_1000_to_600.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_DIB, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 100000, 60000)\n",
    "                \n",
    "                CAPE_DIBDBL_1000_to_600.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_DIBDBL, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 100000, 60000)\n",
    "\n",
    "                CAPE_NOMIX_1000_to_600.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_NOMIX, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 100000, 60000)\n",
    "                \n",
    "                \n",
    "                CAPE_DIB_1000_to_850.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_DIB, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 100000, 85000)\n",
    "                \n",
    "                CAPE_DIBDBL_1000_to_850.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_DIBDBL, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 100000, 85000)\n",
    "\n",
    "                CAPE_NOMIX_1000_to_850.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_NOMIX, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 100000, 85000)\n",
    "                \n",
    "                \n",
    "                CAPE_DIB_850_to_600.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_DIB, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 85000, 60000)\n",
    "                \n",
    "                CAPE_DIBDBL_850_to_600.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_DIBDBL, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 85000, 60000)\n",
    "\n",
    "                CAPE_NOMIX_850_to_600.loc[dict(lat=latitude,lon=longitude)] = calculate_CAPE(temp_v_env, temp_v_plume_NOMIX, true_pressure_midpoint.sel(lat=latitude,lon=longitude), true_pressure_interface.sel(lat=latitude,lon=longitude), 85000, 60000)    \n",
    "        \n",
    "                # Clean up environment #\n",
    "    \n",
    "                gc.collect();\n",
    "      \n",
    "    #################################\n",
    "    ####  Output Data as NetCDF  ####\n",
    "    #################################\n",
    "\n",
    "    # Name variables #\n",
    "\n",
    "    landfrac.name = 'landfrac'\n",
    "    \n",
    "    # Add desired attributes #\n",
    "\n",
    "    landfrac.attrs['Units'] = 'Fraction of land, 0 = all water, 1 = all land'\n",
    "                                                       \n",
    "    # Merge all neccessary dataarrays to a single dataset #\n",
    "\n",
    "    output_dataset = xr.merge([landfrac, CAPE_DIB_1000_to_600, CAPE_DIBDBL_1000_to_600, CAPE_NOMIX_1000_to_600,\\\n",
    "                              CAPE_DIB_1000_to_850, CAPE_DIBDBL_1000_to_850, CAPE_NOMIX_1000_to_850,\\\n",
    "                              CAPE_DIB_850_to_600, CAPE_DIBDBL_850_to_600, CAPE_NOMIX_850_to_600])\n",
    "\n",
    "    # Output dataset to NetCDF #\n",
    "\n",
    "    output_dataset.to_netcdf(fname_datasets[0] + '_' + str(year) + '.nc')\n",
    "    \n",
    "    temp_v_env_all_times.to_netcdf(fname_datasets_temp_v_env[0] + '_' + str(year) + '.nc')\n",
    "    temp_v_plume_DIB_all_times.to_netcdf(fname_datasets_temp_v_plume_DIB[0] + '_' + str(year) + '.nc')\n",
    "    temp_v_plume_DIBDBL_all_times.to_netcdf(fname_datasets_temp_v_plume_DIB[0] + '_' + str(year) + '.nc')\n",
    "    temp_v_plume_NOMIX_all_times.to_netcdf(fname_datasets_temp_v_plume_NOMIX[0] + '_' + str(year) + '.nc')\n",
    "    \n",
    "    ########################\n",
    "    ###  Calculate CSF  ####\n",
    "    ########################\n",
    "    \n",
    "    ####  Calculate Saturation Specific Humidity  ####\n",
    "\n",
    "    print(\"Calculating saturation specific humidity\")\n",
    "\n",
    "    saturation_specific_humidity = xr.apply_ufunc(calculate_saturation_specific_humidity, true_pressure_midpoint, T,\n",
    "                                            output_dtypes=[Q.dtype])\n",
    "\n",
    "    #saturation_specific_humidity_metpy = xr.apply_ufunc(calculate_saturation_specific_humidity_metpy, true_pressure_midpoint, T,\n",
    "    #                                     output_dtypes=[Q.dtype])\n",
    "    \n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "\n",
    "    ####  Column Integrate Variables  ####\n",
    "        \n",
    "    upper_level_integration_limit_Pa = 10000 # [Pa]\n",
    "        \n",
    "    lower_level_integration_limit_Pa = 100000 # [Pa]\n",
    "\n",
    "    print('Column Integrating')\n",
    "        \n",
    "    ci_q, _, _ = mass_weighted_vertical_integral_w_nan(Q, true_pressure_midpoint, true_pressure_interface, lower_level_integration_limit_Pa, upper_level_integration_limit_Pa)\n",
    "    #print(ci_q)\n",
    "    #print(ci_q.min())\n",
    "    #print(ci_q.max())\n",
    "    #print(ci_q.mean())\n",
    "    #plt.figure()\n",
    "    #ci_q.isel(time = 0).plot()\n",
    "        \n",
    "    ci_q_sat, _, _ = mass_weighted_vertical_integral_w_nan(saturation_specific_humidity, true_pressure_midpoint, true_pressure_interface, lower_level_integration_limit_Pa, upper_level_integration_limit_Pa)\n",
    "    #print(ci_q_sat)\n",
    "    #print(ci_q_sat.min())\n",
    "    #print(ci_q_sat.max())\n",
    "    #print(ci_q_sat.mean())\n",
    "    #plt.figure()\n",
    "    #ci_q_sat.isel(time = 0).plot()\n",
    "        \n",
    "    csf = ci_q / ci_q_sat\n",
    "    print(csf)\n",
    "    print(csf.min())\n",
    "    print(csf.max())\n",
    "    plt.figure()\n",
    "    csf.isel(time = 0).plot()\n",
    "    \n",
    "    # Name variables #\n",
    "\n",
    "    csf.name = 'csf'\n",
    "    csf.attrs['Units'] = '[Kg Kg^-1]'\n",
    "\n",
    "    # Clean up environment #\n",
    "        \n",
    "    gc.collect();\n",
    "    \n",
    "    #################################\n",
    "    ####  Output Data as NetCDF  ####\n",
    "    #################################\n",
    "\n",
    "    # Output dataset to NetCDF #\n",
    "\n",
    "    csf.to_netcdf(fname_datasets_CSF[0] + '_' + str(year) + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
