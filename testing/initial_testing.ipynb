{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import sys\n",
    "from glob import glob\n",
    "import gc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../../')\n",
    "\"\"\"\n",
    "local scripts, if loading from a different directory include that with a '.' between\n",
    "directory name and script name\n",
    "\"\"\"\n",
    "from tropical_PODs.PODs.POD_utils import calculate_saturation_specific_humidity\n",
    "from tropical_PODs.PODs.POD_utils import mass_weighted_vertical_integral_w_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define input directories and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years to analyze\n",
    "start_year = (2011)\n",
    "end_year = (2011)\n",
    "\n",
    "################\n",
    "###.  ERA5.  ###\n",
    "################\n",
    "\n",
    "# Atmosphere\n",
    "\n",
    "ifile_specific_humidity = '../../tropical_PODs/data/shum.2p5.*.nc' # ERA5 Specific Humidity\n",
    "ifile_temperature = '../../tropical_PODs/data/air.2p5.*.nc' # ERA5 Temperature\n",
    "ifile_surface_pressure = '../../tropical_PODs/data/pres.sfc.2p5.*.nc' # ERA5 Surface Pressure\n",
    "ifile_precipitation = '../../tropical_PODs/data/3B-DAY.MS.MRG.3IMERG.V06.*' # IMERG Precipitation\n",
    "\n",
    "# Land\n",
    "ifile_land_frac = '../../tropical_PODs/data/land_sea_mask.erai.2p5.nc' # ERAi Land Fraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define output directories and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for datasets string list\n",
    "odir_datasets = '../../tropical_PODs/examples/ofiles_examples/' # ERA5 2p5_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/wpw0gwg52_j941w9vgbh7k2w0000gn/T/ipykernel_24689/2949231961.py:122: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'julian', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  precipitation_rate['time'] = precipitation_rate.indexes['time'].to_datetimeindex() # IMERG time was saved as CFtime, and we need to convert to datetime for xarray\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "    \n",
    "g = 9.8 # [m s^-2]\n",
    "\n",
    "#########################################\n",
    "# Define paths of files we wish to load #\n",
    "#########################################\n",
    "    \n",
    "# glob expands paths with * to a list of files, like the unix shell #\n",
    "\n",
    "paths_specific_humidity = glob(ifile_specific_humidity)\n",
    "paths_temperature = glob(ifile_temperature)\n",
    "paths_surface_pressure = glob(ifile_surface_pressure)\n",
    "paths_precipitation = glob(ifile_precipitation)\n",
    "paths_land = glob(ifile_land_frac)\n",
    "        \n",
    "for year in range(start_year, end_year + 1):\n",
    "        \n",
    "    print(year)\n",
    "            \n",
    "    # Define year strings #\n",
    "        \n",
    "    previous_year_string = str(year - 1)\n",
    "    current_year_string = str(year)\n",
    "    next_year_string = str(year + 1)\n",
    "            \n",
    "    while len(previous_year_string) < 4:\n",
    "        previous_year_string = '0' + previous_year_string\n",
    "                \n",
    "    while len(current_year_string) < 4:\n",
    "        current_year_string = '0' + current_year_string\n",
    "                \n",
    "    while len(next_year_string) < 4:\n",
    "        next_year_string = '0' + next_year_string\n",
    "            \n",
    "    # Limit paths #\n",
    "        \n",
    "    year_limited_paths_specific_humidity = []\n",
    "    year_limited_paths_temperature = []\n",
    "    year_limited_paths_surface_pressure = []\n",
    "    year_limited_paths_precipitation = []\n",
    "            \n",
    "    for string in paths_specific_humidity:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_specific_humidity += [string]\n",
    "            \n",
    "    for string in paths_temperature:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_temperature += [string]\n",
    "            \n",
    "    for string in paths_surface_pressure:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_surface_pressure += [string]\n",
    "                \n",
    "    for string in paths_precipitation:\n",
    "                        \n",
    "        if (current_year_string in string):\n",
    "                \n",
    "            year_limited_paths_precipitation += [string]\n",
    "    \n",
    "    #####################\n",
    "    ####  Load Data  ####\n",
    "    #####################\n",
    "\n",
    "    # Data is \"lazy loaded\", nothing is actually loaded until we \"look\" at data in some way #\n",
    "\n",
    "    dataset_specific_humidity = xr.open_mfdataset(year_limited_paths_specific_humidity, combine=\"by_coords\")\n",
    "    dataset_temperature = xr.open_mfdataset(year_limited_paths_temperature, combine=\"by_coords\")\n",
    "    dataset_surface_pressure = xr.open_dataset(year_limited_paths_surface_pressure[0])\n",
    "    dataset_precipitation = xr.open_mfdataset(year_limited_paths_precipitation, combine=\"by_coords\")\n",
    "    dataset_land = xr.open_dataset(paths_land[0])\n",
    "\n",
    "    #####################\n",
    "    ####  Load Data  ####\n",
    "    #####################\n",
    "              \n",
    "    # Make data arrays, loading only the year of interest #\n",
    "    full_lat = dataset_surface_pressure['lat']\n",
    "    full_lon = dataset_surface_pressure['lon']\n",
    "    land_sea_mask = dataset_land['land_sea_mask']\n",
    "\n",
    "    PS = dataset_surface_pressure['pres'].sel(time = slice(previous_year_string+'-12-31', next_year_string+'-01-01'), lat = slice(10, -10)) # [Pa]\n",
    "    Q = dataset_specific_humidity['shum'].sel(time = slice(previous_year_string+'-12-31', next_year_string+'-01-01'),lat = slice(10, -10), level = slice(70, 1000)) # [Kg/Kg]\n",
    "    T = dataset_temperature['air'].sel(time = slice(previous_year_string+'-12-31', next_year_string+'-01-01'),lat = slice(10, -10), level = slice(70, 1000)) # [K]\n",
    "    precipitation_rate = dataset_precipitation['precipAvg'].sel(time = slice(previous_year_string+'-12-31', next_year_string+'-01-01'), lat = slice(-10, 10)) * (24) # Currently [mm/hr]. Convert to [mm/day]\n",
    "\n",
    "    # Actually load data #\n",
    "    land_sea_mask.load()\n",
    "    PS.load()\n",
    "    Q.load()\n",
    "    T.load()\n",
    "    precipitation_rate.load()\n",
    "\n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    " \n",
    "    ################################\n",
    "    ####  Average Data to Daily ####\n",
    "    ################################\n",
    "    \n",
    "    ###   Test for Averaging Method   ###\n",
    "            \n",
    "    #PS.sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).plot()\n",
    "    #print(PS.resample(time='1D').mean('time').sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75))\n",
    "    #print(PS.sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).mean('time'))\n",
    "    #print(PS.resample(time='1D').mean('time').sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).values == PS.sel(time=slice('1998-01-01','1998-01-01'),lat=5,lon=75).mean('time').values)\n",
    "    \n",
    "    ###   Perform Averaging   ###\n",
    "            \n",
    "    PS = PS.resample(time='1D').mean('time')\n",
    "    Q = Q.resample(time='1D').mean('time')\n",
    "    T = T.resample(time='1D').mean('time')\n",
    "    precipitation_rate = precipitation_rate.resample(time='1D').mean('time')\n",
    "\n",
    "    precipitation_rate['time'] = precipitation_rate.indexes['time'].to_datetimeindex() # IMERG time was saved as CFtime, and we need to convert to datetime for xarray\n",
    "\n",
    "    # ### Update time to reflect center of daily average   ###\n",
    "    \n",
    "    # PS = PS.assign_coords({'time':PS['time']+pd.to_timedelta(10.5, unit='H')})\n",
    "    # Q = Q.assign_coords({'time':Q['time']+pd.to_timedelta(10.5, unit='H')})\n",
    "    # T = T.assign_coords({'time':T['time']+pd.to_timedelta(10.5, unit='H')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'air' (time: 365, level: 28, lat: 9, lon: 144)>\n",
      "array([[[[194.76683, 194.78355, 195.18907, ..., 195.5117 , 195.2788 ,\n",
      "          194.97557],\n",
      "         [195.58446, 196.01256, 196.55687, ..., 196.5554 , 196.24864,\n",
      "          195.91283],\n",
      "         [197.46617, 197.82664, 197.57896, ..., 197.98375, 197.68016,\n",
      "          197.22826],\n",
      "         ...,\n",
      "         [200.12938, 199.89183, 199.4818 , ..., 200.1964 , 199.81944,\n",
      "          200.0161 ],\n",
      "         [199.7307 , 199.79503, 199.67833, ..., 199.14049, 199.01964,\n",
      "          199.41734],\n",
      "         [198.60997, 198.56541, 199.06664, ..., 198.09691, 198.15404,\n",
      "          198.49779]],\n",
      "\n",
      "        [[195.3222 , 195.3377 , 195.00018, ..., 195.95612, 195.43024,\n",
      "          195.33478],\n",
      "         [195.16656, 194.76129, 194.4079 , ..., 195.35431, 195.25311,\n",
      "          195.27203],\n",
      "         [194.23444, 193.69586, 193.2829 , ..., 194.13788, 194.25006,\n",
      "          194.26898],\n",
      "...\n",
      "         [294.32788, 293.9131 , 294.1748 , ..., 294.06177, 294.0005 ,\n",
      "          294.29492],\n",
      "         [292.96484, 293.18896, 292.84888, ..., 293.52783, 293.14648,\n",
      "          293.18164],\n",
      "         [291.82642, 292.02954, 291.74268, ..., 292.69995, 292.32056,\n",
      "          291.80542]],\n",
      "\n",
      "        [[301.11514, 302.04825, 303.9086 , ..., 298.42056, 299.51358,\n",
      "          300.0768 ],\n",
      "         [302.2526 , 302.18887, 301.4977 , ..., 302.0961 , 302.1901 ,\n",
      "          302.17007],\n",
      "         [299.14908, 299.5363 , 298.68472, ..., 298.13394, 298.4142 ,\n",
      "          298.97647],\n",
      "         ...,\n",
      "         [296.48306, 296.07364, 296.33585, ..., 296.2001 , 296.1437 ,\n",
      "          296.43594],\n",
      "         [295.09854, 295.3383 , 295.00015, ..., 295.66397, 295.2736 ,\n",
      "          295.30167],\n",
      "         [293.94766, 294.16006, 293.8517 , ..., 294.83536, 294.43668,\n",
      "          293.91568]]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * level    (level) float64 70.0 100.0 125.0 150.0 ... 925.0 950.0 975.0 1e+03\n",
      "  * lat      (lat) float32 10.0 7.5 5.0 2.5 0.0 -2.5 -5.0 -7.5 -10.0\n",
      "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * time     (time) datetime64[ns] 2011-01-01 2011-01-02 ... 2011-12-31\n",
      "Attributes: (12/15)\n",
      "    GridType:                      Cylindrical Equidistant Projection Grid\n",
      "    datum:                         wgs84\n",
      "    long_name:                     3-hourly Air Temperature on Pressure Levels\n",
      "    units:                         K\n",
      "    var_desc:                      Air temperature\n",
      "    standard_name:                 air_temperature\n",
      "    ...                            ...\n",
      "    parent_stat:                   Other\n",
      "    number_of_significant_digits:  7\n",
      "    ecmwf_local_table:             128\n",
      "    ecmwf_parameter:               130\n",
      "    actual_range:                  [176.64613 322.21252]\n",
      "    remap:                         remapped via ESMF_regrid_with_weights: Bil...\n",
      "<xarray.DataArray 'precipAvg' (time: 365, lat: 9, lon: 144)>\n",
      "array([[[1.68504443e-03, 7.51780243e-04, 1.90737619e-04, ...,\n",
      "         6.16566206e-02, 4.78880055e-02, 9.65026910e-04],\n",
      "        [0.00000000e+00, 1.68443877e-05, 9.97800955e-02, ...,\n",
      "         7.06779127e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "        [1.50765658e-01, 3.40161580e-01, 1.18459803e+01, ...,\n",
      "         1.35016613e-04, 1.73191307e-04, 5.73116934e-03],\n",
      "        ...,\n",
      "        [5.91244757e-02, 7.72950217e-03, 1.03282438e-01, ...,\n",
      "         1.40758528e-01, 2.23610406e-01, 3.06983825e-01],\n",
      "        [7.09181039e-02, 6.29739077e-03, 1.27563698e-02, ...,\n",
      "         2.00341977e-01, 1.55387685e-01, 8.97186853e-02],\n",
      "        [4.56669339e-03, 4.62162825e-04, 1.47999247e-03, ...,\n",
      "         8.53728583e-02, 3.26722271e-02, 1.07643250e-02]],\n",
      "\n",
      "       [[3.21061038e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "         2.00063986e-02, 1.14546333e-02, 2.83765189e-03],\n",
      "        [0.00000000e+00, 0.00000000e+00, 4.09727585e-02, ...,\n",
      "         1.16539168e-05, 5.47060241e-03, 0.00000000e+00],\n",
      "        [4.08193715e-04, 1.48146132e-01, 5.27800114e-01, ...,\n",
      "         9.44588584e-04, 4.18597143e-02, 2.11848514e-02],\n",
      "...\n",
      "        [1.19082152e-01, 2.25085791e-02, 5.07713293e-02, ...,\n",
      "         8.70950792e+00, 5.20435357e-01, 9.08923156e-02],\n",
      "        [1.88788053e-03, 1.10815169e-03, 7.91207479e-03, ...,\n",
      "         1.02900832e+00, 8.76836568e-02, 6.26278425e-03],\n",
      "        [5.33169043e-03, 6.59092618e-04, 4.14881063e-04, ...,\n",
      "         1.04955526e-02, 1.17915806e-02, 6.29866582e-04]],\n",
      "\n",
      "       [[4.83336483e-03, 3.89567134e-05, 0.00000000e+00, ...,\n",
      "         1.96841525e-04, 0.00000000e+00, 9.32989062e-03],\n",
      "        [2.08379915e-02, 7.85843796e-05, 0.00000000e+00, ...,\n",
      "         1.70951612e-04, 3.01216109e-03, 3.13359805e-02],\n",
      "        [6.79255277e-02, 1.38857616e-02, 9.12337849e-03, ...,\n",
      "         7.82343130e-03, 5.01006847e-02, 1.07507276e-01],\n",
      "        ...,\n",
      "        [2.12590331e-05, 7.82719741e-01, 1.38812077e+01, ...,\n",
      "         8.33285473e-01, 5.49524048e-02, 3.99211891e-03],\n",
      "        [5.13571151e-04, 1.59524841e-04, 1.74395194e-01, ...,\n",
      "         3.56813455e-01, 5.12760517e-02, 2.66875031e-02],\n",
      "        [4.29232702e-03, 2.71895540e-05, 9.99646880e-04, ...,\n",
      "         6.76937704e-03, 1.88324373e-03, 1.26058608e-03]]])\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0\n",
      "  * lon      (lon) float64 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
      "  * time     (time) datetime64[ns] 2011-01-01 2011-01-02 ... 2011-12-31\n"
     ]
    }
   ],
   "source": [
    "print(T)\n",
    "print(precipitation_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ###############################################\n",
    "    ####  Modify \"landfrac\" Variable as Needed ####\n",
    "    ###############################################\n",
    "    \n",
    "    print(\"Modifying landfrac as needed\")\n",
    "\n",
    "    landfrac = land_sea_mask.rename({'Latitude':'lat','Longitude':'lon'})\n",
    "    landfrac = landfrac.rename('landfrac')\n",
    "    print(landfrac)\n",
    "    \n",
    "    # The landfrac variable does not have lat/lon coordinates. Assign those of variables and check to make sure they make sense #\n",
    "    \n",
    "    #print(landfrac.coords['lat'])\n",
    "    landfrac.coords['lat'] = full_lat.coords['lat']\n",
    "    landfrac.coords['lon'] = full_lon.coords['lon']\n",
    "\n",
    "    landfrac = landfrac.transpose()\n",
    "    \n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "    \n",
    "    #####################################\n",
    "    ####  Modify variables as needed ####\n",
    "    #####################################\n",
    "    \n",
    "    PS = PS.rename('PS')\n",
    "    PS = PS.transpose('time','lat','lon')\n",
    "    PS = PS.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    #print(PS)\n",
    "    \n",
    "    Q = Q.rename({'level':'lev'})\n",
    "    Q = Q.rename('Q')\n",
    "    Q = Q.transpose('time','lev','lat','lon')\n",
    "    Q = Q.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    #print(Q)\n",
    "    \n",
    "    T = T.rename({'level':'lev'})\n",
    "    T = T.rename('T')\n",
    "    T = T.transpose('time','lev','lat','lon')\n",
    "    T = T.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    #print(T)\n",
    "\n",
    "    precipitation_rate = precipitation_rate.rename('precipitation_rate')\n",
    "    precipitation_rate = precipitation_rate.transpose('time','lat','lon')\n",
    "    precipitation_rate = precipitation_rate.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    \n",
    "    landfrac = landfrac.sortby('lat', ascending=True) # Re-order lat to match code for other datasets\n",
    "    \n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "\n",
    "    #########################################\n",
    "    ####  Calculate True Model Pressure  ####\n",
    "    #########################################\n",
    "\n",
    "    print(\"Calculating true model pressure\")\n",
    "    \n",
    "    # Set upper most interface equal to uppermost level midpoint, and lowest interface equal to surface pressure.\n",
    "    # This will still permit the desired vertical integral, just choose appropriate upper and lower integration limits\n",
    "    \n",
    "    # Model level midpoint\n",
    "\n",
    "    true_pressure_midpoint = Q['lev'] * 100. # To convert to Pa\n",
    "    true_pressure_midpoint = true_pressure_midpoint.rename('true_pressure_midpoint_Pa')\n",
    "    true_pressure_midpoint = true_pressure_midpoint.expand_dims({'lat':Q['lat'], 'lon':Q['lon'], 'time':Q['time']})\n",
    "    true_pressure_midpoint = true_pressure_midpoint.transpose('time','lev','lat','lon')\n",
    "    \n",
    "    # Model level interfaces\n",
    "    \n",
    "    true_pressure_interface = np.empty((len(Q.time),len(Q.lat),len(Q.lon),len(Q.lev)+1))\n",
    "\n",
    "    for interface_level_counter in range(len(Q.lev) + 1):\n",
    "        if interface_level_counter == 0:\n",
    "            true_pressure_interface[:,:,:,interface_level_counter] = Q['lev'].isel(lev=0).values # Set upper most interface equal to uppermost level midpoint\n",
    "        elif interface_level_counter == (len(Q.lev)):\n",
    "            true_pressure_interface[:,:,:,interface_level_counter] = PS # Set lowest interface equal to surface pressure\n",
    "        else:\n",
    "            true_pressure_interface[:,:,:,interface_level_counter] = (Q['lev'].isel(lev=interface_level_counter-1).values + Q['lev'].isel(lev=interface_level_counter).values) / 2.,  # Set middle interfaces equal to half way points between level midpoints\n",
    "            \n",
    "    coords = {'time':Q['time'], 'lat':Q['lat'], 'lon':Q['lon'], 'ilev':np.arange(1,len(Q.lev) + 2)}\n",
    "    dims = ['time', 'lat', 'lon', 'ilev']\n",
    "    true_pressure_interface = xr.DataArray(true_pressure_interface,dims=dims,coords=coords) * 100. # To convert to Pa\n",
    "    true_pressure_interface.attrs['units'] = 'Pa'      \n",
    "    \n",
    "    true_pressure_interface = true_pressure_interface.transpose('time','ilev','lat','lon')\n",
    "\n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "  \n",
    "    ########################\n",
    "    ###  Calculate CSF  ####\n",
    "    ########################\n",
    "    \n",
    "    ####  Calculate Saturation Specific Humidity  ####\n",
    "\n",
    "    print(\"Calculating saturation specific humidity\")\n",
    "\n",
    "    saturation_specific_humidity = xr.apply_ufunc(calculate_saturation_specific_humidity, true_pressure_midpoint, T,\n",
    "                                            output_dtypes=[Q.dtype])\n",
    "    \n",
    "    # Clean up environment #\n",
    "    \n",
    "    gc.collect();\n",
    "\n",
    "    ####  Column Integrate Variables  ####\n",
    "        \n",
    "    upper_level_integration_limit_Pa = 10000 # [Pa]\n",
    "        \n",
    "    lower_level_integration_limit_Pa = 100000 # [Pa]\n",
    "\n",
    "    print('Column Integrating')\n",
    "        \n",
    "    ci_q, _, _ = mass_weighted_vertical_integral_w_nan(Q, true_pressure_midpoint, true_pressure_interface, lower_level_integration_limit_Pa, upper_level_integration_limit_Pa)\n",
    "    #print(ci_q)\n",
    "    #print(ci_q.min())\n",
    "    #print(ci_q.max())\n",
    "    #print(ci_q.mean())\n",
    "    #plt.figure()\n",
    "    #ci_q.isel(time = 0).plot()\n",
    "        \n",
    "    ci_q_sat, _, _ = mass_weighted_vertical_integral_w_nan(saturation_specific_humidity, true_pressure_midpoint, true_pressure_interface, lower_level_integration_limit_Pa, upper_level_integration_limit_Pa)\n",
    "    #print(ci_q_sat)\n",
    "    #print(ci_q_sat.min())\n",
    "    #print(ci_q_sat.max())\n",
    "    #print(ci_q_sat.mean())\n",
    "    #plt.figure()\n",
    "    #ci_q_sat.isel(time = 0).plot()\n",
    "        \n",
    "    csf = ci_q / ci_q_sat\n",
    "    print(csf)\n",
    "    print(csf.min())\n",
    "    print(csf.max())\n",
    "    plt.figure()\n",
    "    csf.isel(time = 0).plot()\n",
    "    \n",
    "    # Name variables #\n",
    "\n",
    "    csf.name = 'csf'\n",
    "    csf.attrs['Units'] = '[Kg Kg^-1]'\n",
    "\n",
    "    # Clean up environment #\n",
    "\n",
    "    del Q, T, true_pressure_midpoint, true_pressure_interface, saturation_specific_humidity, ci_q, ci_q_sat\n",
    "        \n",
    "    gc.collect();\n",
    "   \n",
    "    #################################\n",
    "    ####  Output Data as NetCDF  ####\n",
    "    #################################\n",
    "\n",
    "    # Output dataset to NetCDF #\n",
    "\n",
    "    csf.sel(time = slice(current_year_string+'-01-01', current_year_string+'-12-31').to_netcdf(odir_datasets + 'initial_test_CSF_' + current_year_string + '.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "        ####  Limit to Oceanic (<10% Land) Points  ####\n",
    "        ###############################################\n",
    "        \n",
    "        print('Applying Land/Ocean Mask')\n",
    "        \n",
    "        # Create ocean mask #\n",
    "\n",
    "        is_valid_ocean_mask = (landfrac < 0.1)\n",
    "#         is_valid_ocean_mask_TRMM_2A23 = (landfrac.interp_like(TRMM_2A23_shallow) < 0.1)\n",
    "\n",
    "        #is_valid_ocean_mask.plot()\n",
    "\n",
    "        # Apply ocean mask to appropriate variables, setting invalid locations to nan #\n",
    "        \n",
    "        precipitation_rate = precipitation_rate.where(is_valid_ocean_mask, other = np.nan)\n",
    "        \n",
    "#         rain_MCS = rain_MCS.where(is_valid_ocean_mask, other = np.nan)\n",
    "#         rain_nonMCS = rain_nonMCS.where(is_valid_ocean_mask, other = np.nan)\n",
    "#         rain_nonDeep = rain_nonDeep.where(is_valid_ocean_mask, other = np.nan)\n",
    "        \n",
    "#         TRMM_2A23_shallow = TRMM_2A23_shallow.where(is_valid_ocean_mask_TRMM_2A23, other = np.nan)\n",
    "#         TRMM_2A23_conv = TRMM_2A23_conv.where(is_valid_ocean_mask_TRMM_2A23, other = np.nan)\n",
    "#         TRMM_2A23_strat = TRMM_2A23_strat.where(is_valid_ocean_mask_TRMM_2A23, other = np.nan)\n",
    "\n",
    "        CAPE_DIB_1000_to_600 = CAPE_DIB_1000_to_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        CAPE_DIBDBL_1000_to_600 = CAPE_DIBDBL_1000_to_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        CAPE_NOMIX_1000_to_600 = CAPE_NOMIX_1000_to_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        \n",
    "        T_1000_hPa= T_1000_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        Q_1000_hPa= Q_1000_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        DSE_1000_hPa= DSE_1000_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        MSE_1000_hPa= MSE_1000_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        T_850_hPa= T_850_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        Q_850_hPa= Q_850_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        DSE_850_hPa= DSE_850_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        MSE_850_hPa= MSE_850_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        T_600_hPa= T_600_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        Q_600_hPa= Q_600_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        DSE_600_hPa= DSE_600_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        MSE_600_hPa= MSE_600_hPa.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_T_1000_850= mwa_T_1000_850.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_Q_1000_850= mwa_Q_1000_850.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_DSE_1000_850= mwa_DSE_1000_850.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_MSE_1000_850= mwa_MSE_1000_850.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_T_850_600= mwa_T_850_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_Q_850_600= mwa_Q_850_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_DSE_850_600= mwa_DSE_850_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_MSE_850_600= mwa_MSE_850_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_T_1000_950= mwa_T_1000_950.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_Q_1000_950= mwa_Q_1000_950.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_DSE_1000_950= mwa_DSE_1000_950.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_MSE_1000_950= mwa_MSE_1000_950.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_T_950_800= mwa_T_950_800.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_Q_950_800= mwa_Q_950_800.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_DSE_950_800= mwa_DSE_950_800.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_MSE_950_800= mwa_MSE_950_800.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_T_800_600= mwa_T_800_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_Q_800_600= mwa_Q_800_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_DSE_800_600= mwa_DSE_800_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_MSE_800_600= mwa_MSE_800_600.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_T_600_300= mwa_T_600_300.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_Q_600_300= mwa_Q_600_300.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_DSE_600_300= mwa_DSE_600_300.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_MSE_600_300= mwa_MSE_600_300.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_T_300_100= mwa_T_300_100.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_Q_300_100= mwa_Q_300_100.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_DSE_300_100= mwa_DSE_300_100.where(is_valid_ocean_mask, other = np.nan)\n",
    "        mwa_MSE_300_100= mwa_MSE_300_100.where(is_valid_ocean_mask, other = np.nan)\n",
    "                \n",
    "        csf = csf.where(is_valid_ocean_mask, other = np.nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
